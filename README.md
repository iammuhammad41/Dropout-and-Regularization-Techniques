# Dropout-and-Regularization-Techniques
Dropout is a well-known technique where random units are dropped during training to prevent overfitting. Other regularization techniques like L2 regularization (weight decay) or data augmentation are used to improve generalization and reduce overfitting.
